{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data retrieval Ebay Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Craigslist scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import get to call a get request on the site\n",
    "from requests import get\n",
    "#get the first page of the east bay housing prices\n",
    "response = get('https://sfbay.craigslist.org/search/eby/apa?hasPic=1&availabilityMode=0') #get rid of those lame-o's that post a housing option without a pic using their filter\n",
    "from bs4 import BeautifulSoup\n",
    "html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "#get the macro-container for the housing posts\n",
    "posts = html_soup.find_all('li', class_= 'result-row')\n",
    "print(type(posts)) #to double check that I got a ResultSet\n",
    "print(len(posts)) #to double check I got 120 (elements/page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ebay scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# List of item names to search on eBay\n",
    "name_list = [\"Ramen\", \"Monster Hunter World\", \"Adhesive page markers\", \"Calculator\", \"arduino\", \"gtx 1070\",\n",
    "             \"bluetooth headphones\", \"coffee machine\", \"sweet tea\", \"Python textbook\"]\n",
    "\n",
    "\n",
    "# Returns a list of urls that search eBay for an item\n",
    "def make_urls(names):\n",
    "    # eBay url that can be modified to search for a specific item on eBay\n",
    "    url1 = \"https://www.ebay.de/sch/i.html?_odkw=\"\n",
    "    url2 = \"&_osacat=0&_from=R40&_trksid=p2045573.m570.l1313.TR12.TRC2.A0.H0.X\"\n",
    "    url3 = \".TRS0&_nkw=\"\n",
    "    url4 = \"&_sacat=0\"\n",
    "    # List of urls created\n",
    "    urls = []\n",
    "\n",
    "    for name in names:\n",
    "        # Adds the name of item being searched to the end of the eBay url and appends it to the urls list\n",
    "        # In order for it to work the spaces need to be replaced with a +\n",
    "        urls.append(url1 + name.replace(\" \", \"+\") + url2 + name.replace(\" \", \"+\") + url3 + name.replace(\" \", \"+\") + url4)\n",
    "    # Returns the list of completed urls\n",
    "    return urls\n",
    "\n",
    "\n",
    "# Scrapes and prints the url, name, and price of the first item result listed on eBay\n",
    "def ebay_scrape(urls):\n",
    "    for url in urls:\n",
    "        # Downloads the eBay page for processing\n",
    "        res = requests.get(url, timeout=5)\n",
    "        # Raises an exception error if there's an error downloading the website\n",
    "        res.raise_for_status()\n",
    "        # Creates a BeautifulSoup object for HTML parsing\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        # Scrapes the first listed item's name\n",
    "        name = soup.find(\"h3\", {\"class\": \"lvtitle\"}).get_text(separator=u\" \")\n",
    "        # Scrapes the first listed item's price\n",
    "        price = soup.find(\"span\", {\"class\": \"bold\"}).get_text()\n",
    "\n",
    "        # Prints the url, listed item name, and the price of the item\n",
    "        print(url)\n",
    "        print()\n",
    "        print(\"Item Name: \" + name)\n",
    "        print(\"Price: \" + price + \"\\n\")\n",
    "\n",
    "\n",
    "# Runs the code\n",
    "# 1. Make the eBay url list\n",
    "# 2. Use the returned url list to search eBay and scrape and print information on each item\n",
    "ebay_scrape(make_urls(name_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ebay Kleinanzeigen scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import regex as re\n",
    "import pandas as ps\n",
    "\n",
    "\n",
    "# List of item names to search on eBay\n",
    "name_list = [\"auto\", \"wohnung\"]\n",
    "\n",
    "city_list = [\"s-berlin\", \"s-koeln\"]\n",
    "\n",
    "\n",
    "# Returns a list of urls that search eBay for an item\n",
    "def make_urls(names, cities):\n",
    "    # eBay url that can be modified to search for a specific item on eBay\n",
    "    url1 = \"https://www.ebay-kleinanzeigen.de/\"\n",
    "    # List of urls created\n",
    "    urls = []\n",
    "\n",
    "    for name in names:\n",
    "        for city in cities:\n",
    "        # Adds the name of item being searched to the end of the eBay url and appends it to the urls list\n",
    "        # In order for it to work the spaces need to be replaced with a +\n",
    "            if (city == \"s-berlin\"):\n",
    "                urls.append(url1 + city.replace(\" \", \"+\") + \"/anzeige:angebote/\" + name.replace(\" \", \"+\") + \"/\" + \"k0l3331\")\n",
    "            elif (city == \"s-koeln\"):\n",
    "                urls.append(url1 + city.replace(\" \", \"+\") + \"/anzeige:angebote/\" + name.replace(\" \", \"+\") + \"/\" + \"k0l945\")\n",
    "            else:\n",
    "                print(\"error\")\n",
    "            \n",
    "    # Returns the list of completed urls\n",
    "    return urls\n",
    "\n",
    "\n",
    "# Scrapes and prints the url, name, and price of the first item result listed on eBay\n",
    "def ebay_scrape(urls):\n",
    "    title_list = []\n",
    "    price_list = []\n",
    "    for url in urls: \n",
    "        \n",
    "        # Downloads the eBay page for processing\n",
    "        res = requests.get(url, timeout=5)\n",
    "        # Raises an exception error if there's an error downloading the website\n",
    "        res.raise_for_status()\n",
    "        # Creates a BeautifulSoup object for HTML parsing\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        names = soup.find_all(\"h2\", {\"class\": \"text-module-begin\"})\n",
    "        prices = soup.find_all(\"div\", {\"class\": \"aditem-details\"})\n",
    "        print( prices[1].get_text(separator=u\" \"))\n",
    "#         print(names[1])\n",
    "        for i in range(20):\n",
    "            # Scrapes the first listed item's name\n",
    "            name = names[i].get_text(separator=u\" \")\n",
    "            # Build lists with appended items\n",
    "            title_list.append(name)\n",
    "            \n",
    "            # Scrapes the first listed item's price\n",
    "            price = prices[i].get_text(separator=u\" \")\n",
    "            head, sep, tail = price.partition(' ')\n",
    "            head, sep, tail = tail.partition(' ')\n",
    "            price_list.append(head)\n",
    "\n",
    "            \n",
    "            \n",
    "        \n",
    "#         # Prints the url, listed item name, and the price of the item\n",
    "#         print()\n",
    "#         print(\"Item Name: \" + name)\n",
    "#         print(\"Price: \" + price + \"\\n\")\n",
    "    # Convert lists to pandas dataframe\n",
    "#     print(titles)\n",
    "#     print(prices)\n",
    "    \n",
    "    d = {'title': title_list, 'prices':price_list}\n",
    "        \n",
    "    df = pd.DataFrame(d)\n",
    "        \n",
    "\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 4.500 € VB \n",
      " \n",
      "                    12107 \n",
      "                        Tempelhof \n",
      "\n",
      "\n",
      " 6.200 € VB \n",
      " \n",
      "                    50765 \n",
      "                        Blumenberg \n",
      "\n",
      "\n",
      " 459.000 € \n",
      " \n",
      "                    10407 \n",
      "                        Prenzlauer Berg \n",
      "\n",
      "\n",
      "  VB \n",
      " \n",
      "                    50667 \n",
      "                        Köln Altstadt \n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = ebay_scrape(make_urls(name_list, city_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                title   prices\n",
      "0   \\n Nur 180 Mietwagen Auto Mieten Berlin Autove...       25\n",
      "1   \\n VW Golf V 1,6L•TÜV-1/2021•Klima•4/5Türig•E-...    4.500\n",
      "2                  \\n Opel Vectra A 1.6i TÜV 11/20 \\n      895\n",
      "3   \\n AUVELO Autotransport | Fahrzeug | Auto | Tr...         \n",
      "4                                  \\n Bmw e90 320i \\n    6.200\n",
      "5                                \\n Fiat Punto 1.2 \\n    1.250\n",
      "6         \\n offene Terrasse,wunderbar zum sonnen. \\n      600\n",
      "7   \\n ***Provisionsfrei - 4 Zimmer - vermietet - ...  459.000\n",
      "8   \\n Wir reinigen Büros, Praxen, Wohnungen, Häus...         \n",
      "9   \\n Denkmal-Wohnung nahe Köln // Attraktive Ste...  323.000\n",
      "10  \\n ✅ Maler Malerarbeiten Wohnung streichen, 50...         \n",
      "11  \\n Schauspieler sucht 1-2 Zimmer-Wohnung in Kö...      400\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
